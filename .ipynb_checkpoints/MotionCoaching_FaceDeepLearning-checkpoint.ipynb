{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 공통 요소 ###\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "path = \"C:/Users/NAM WOO JIN/Downloads/SpeechTextClassifier_hansung\"\n",
    "os.chdir(path)\n",
    "\n",
    "def read_data(filename, encoding):\n",
    "    \"\"\"읽기 함수\"\"\"\n",
    "    with open(filename, 'r', encoding=encoding) as f:\n",
    "        data = [line.split('\\t') for line in f.read().splitlines()]\n",
    "        data = data[0:]                 # txt 파일의 헤더(id document label)는 제외하기는 1:\n",
    "    return data\n",
    "\n",
    "\n",
    "def write_data_list(list, filename, encoding):\n",
    "    \"\"\"쓰기 함수\"\"\"\n",
    "    with open(filename, 'w') as f:\n",
    "        for item in list:\n",
    "            f.write('%s\\t%s\\t%s\\t%s\\n' % (item[0], item[1], item[2], item[3]))\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "### train_test_split() 함수를 이용하여 훈련데이터와 테스트데이터 분리 ###\n",
    "data = read_data('story_morphed_1013.txt', encoding='cp949')\n",
    "train, test, = train_test_split(data, test_size=0.1)\n",
    "\n",
    "write_data_list(list=train, filename='train_story_morphed_1013.txt', encoding='cp949')\n",
    "write_data_list(list=test, filename='test_story_morphed_1013.txt', encoding='cp949')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "def read_data(filename, encoding):\n",
    "    \"\"\"읽기 함수\"\"\"\n",
    "    with open(filename, 'r', encoding=encoding) as f:\n",
    "        data = [line.split('\\t') for line in f.read().splitlines()]\n",
    "        data = data[0:]  # txt 파일의 헤더(id document label)는 제외하기는 1:\n",
    "    return data\n",
    "\n",
    "\n",
    "print('\\n######## Environment Setting ########')\n",
    "filename = 'train_story_morphed_1013'\n",
    "valname = 'test_story_morphed_1013'                         # 테스트 파일이 없어도 이 부분을 지우지 않아도 됨\n",
    "\n",
    "train_file_name = filename+'.txt'                           # 입력할 파일의 이름\n",
    "test_file_name = valname+'.txt'\n",
    "model_name = filename+'.h5'                                 # 저장될 모델의 이름\n",
    "tokenizer_name = filename+'.pickle'                         # 저장될 토크나이저의 이름\n",
    "epochs = 10                                                 # 수행할 에포크의 수\n",
    "\n",
    "\n",
    "os.chdir(path)\n",
    "print(\"Current Directory:\", os.getcwd())\n",
    "\n",
    "data = read_data(train_file_name, encoding='cp949')\n",
    "print('train length:', len(data))\n",
    "texts = [line[2] for line in data]                      # 훈련데이터 본문\n",
    "labels = [line[3] for line in data]                     # 훈련데이터 레이블 부분\n",
    "labels = list(map(int, labels))                         # 문자열을 숫자로 변환한다\n",
    "\n",
    "# 성능측정을 위하여 테스트 데이터를 사용\n",
    "data_val = read_data(test_file_name, encoding='cp949')\n",
    "print('test length:', len(data_val))\n",
    "texts_val = [line[2] for line in data_val]          # 테스트 데이터 본문\n",
    "labels_val = [line[3] for line in data_val]         # 테스트 데이터 레이블 부분\n",
    "labels_val = list(map(int, labels_val))             # 문자열을 숫자로 변환한다\n",
    "\n",
    "\n",
    "\n",
    "print('\\n######## Data Tokenizing ########')\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000)                      # 상위빈도 10,000 개의 단어만을 추려내는 Tokenizer 객체 생성\n",
    "tokenizer.fit_on_texts(texts)                               # 단어 인덱스를 구축한다\n",
    "word_index = tokenizer.word_index                           # 단어 인덱스만 가져온다\n",
    "\n",
    "print('전체에서 %s개의 고유한 토큰을 찾았습니다.' % len(word_index))      # 10,000개가 아닌 전체인 88,582개를 보여준다. texts_to_sequences()를 거쳐야 10,000개만 남는다.\n",
    "print('word_index type: ', type(word_index))\n",
    "\n",
    "\n",
    "print('\\n######## data 변환 ########')\n",
    "data = tokenizer.texts_to_sequences(texts)                  # 상위 빈도 10,000(max_words)개의 단어만 word_index의 숫자 리스트로 변환. Tokenizer 결과가 여기서 반영된다.\n",
    "data = pad_sequences(data, maxlen=100)                      # 길이를 고정시킨다. maxlen의 수만큼으로 2D 텐서를 만든다. 200을 넘는 데이터는 잘라내고, 모자라는 데이터는 0으로 채운다\n",
    "data_val = tokenizer.texts_to_sequences(texts_val)\n",
    "data_val = pad_sequences(data_val, maxlen=100)\n",
    "\n",
    "print('data type:', type(data))\n",
    "print('data length:', len(data))\n",
    "print('texts 0:', texts[0])\n",
    "print('data:', data)\n",
    "print('data 0:', data[0])\n",
    "\n",
    "\n",
    "def to_one_hot(sequences, dimension):                       # 원-핫 인코딩 함수\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "\n",
    "labels = to_one_hot(labels, dimension=3)\n",
    "labels_val = to_one_hot(labels_val, dimension=3)\n",
    "\n",
    "\n",
    "print('\\n######## Train 데이터와 Test 데이터 준비 ########')\n",
    "print('데이터 텐서의 크기:', data.shape)          # (25000, 10000)\n",
    "print('레이블 텐서의 크기:', labels.shape)        # (25000,)\n",
    "\n",
    "x_train = data\n",
    "y_train = labels\n",
    "x_val = data_val\n",
    "y_val = labels_val\n",
    "\n",
    "\n",
    "print('\\n######## Define Model ########')\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Flatten\n",
    "\n",
    "model = Sequential()                                                       # 모델을 새로 정의\n",
    "model.add(Embedding(input_dim=10000, output_dim=100, input_length=100))    # 밀집벡터를 사용하는 임베딩 층. 출력층은 3D가 됨.\n",
    "model.add(Flatten())                                                       # 3D 임베딩 텐서를 2D 텐서로 펼친다.\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(units=3, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "\n",
    "print('\\n######## Compile Model ########')\n",
    "loss = 'categorical_crossentropy'\n",
    "model.compile(optimizer='rmsprop', loss=loss, metrics=['acc'])             # adam 또는 rmsprop을 많이 사용한다\n",
    "\n",
    "\n",
    "\n",
    "print('\\n######## Train Model ########')\n",
    "# 반환값의 history는 훈련하는 동안 발생한 모든 정보를 담고 있는 딕셔너리이다\n",
    "history = model.fit(x_train, y_train, epochs=epochs, batch_size=16, validation_data=(x_val, y_val), verbose=1)\n",
    "history_dict = history.history\n",
    "\n",
    "\n",
    "print('\\n######## Save Model ########')\n",
    "os.chdir(path+\"/model\")\n",
    "import pickle\n",
    "model.save(model_name)\n",
    "\n",
    "with open(tokenizer_name, 'wb') as file:            # 훈련데이터에서 사용된 상위빈도 10,000개의 단어로 된 Tokenizer 저장\n",
    "    pickle.dump(tokenizer, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print('Elapsed seconds:', end - start)\n",
    "print(\"모든 작업이 끝났습니다.\")\n",
    "\n",
    "\n",
    "\n",
    "print('\\n######## Accuracy & Loss ########')\n",
    "# history 딕셔너리 안에 있는 정확도와 손실값을 가져와 본다\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "print('Accuracy of each epoch:', acc)\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "print('\\n######## Plotting Performance ########')\n",
    "# 정확도와 손실값의 변화를 보고, epoch를 어디에서 조절해야 할 지를 가늠한다.\n",
    "# 정확도가 떨어지는 구간, 손실값이 높게 나타나는 구간을 확인한다\n",
    "# 데이터가 큰 경우 대개 epoch를 늘려야 최적값에 도달한다\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training Acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation Acc')\n",
    "plt.title('Training and Validation accuracy')\n",
    "plt.legend(loc=2)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.legend(loc=2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
